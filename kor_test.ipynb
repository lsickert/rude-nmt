{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from rude_nmt import label_korean\n",
    "nlp = spacy.load(\"ko_core_news_lg\", disable=[\"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"그들은 이슬람교도들을 동물에 빗대었습니다. 저는 이거슨 좋아해요.\"\n",
    "test_str_2 = \"빗대었습니다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RE = re.compile(r\"\\w(?:습니다)\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stem': '빗대었습', 'declInd1': '니다', 'declInd2': None, 'declInd3': None, 'declInd4': None, 'declDesc': None, 'cert': None, 'intInd': None, 'intInt': None, 'imp': None, 'prop1': None, 'prop2': None, 'prop3': None, 'req': None}\n",
      "[('빗대었습', '니다', '', '', '', '', '', '', '', '', '', '', '', '')]\n"
     ]
    }
   ],
   "source": [
    "matches = label_korean.HASIPSIOCHE_RE.finditer(test_str)\n",
    "for match in matches:\n",
    "    print(match.groupdict())\n",
    "\n",
    "matches2 = label_korean.HASIPSIOCHE_RE.findall(test_str)\n",
    "print(matches2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_korean.is_hasipsioche(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attributes(doc_or_span):\n",
    "    print(\"Text\\tLemma\\tUPOS\\tXPOS\\tDEP\\tShape\\tIsAlpha\\tIsStop\\tIsPunct\\tIsAscii\\n\" + \"-\"*80)\n",
    "    for token in doc_or_span:\n",
    "        print(\n",
    "            f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\\t\"\n",
    "            f\"{token.dep_}\\t{token.shape_}\\t{token.is_alpha}\\t\"\n",
    "            f\"{token.is_stop}\\t{token.is_punct}\\t{token.is_ascii}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text\tLemma\tUPOS\tXPOS\tDEP\tShape\tIsAlpha\tIsStop\tIsPunct\tIsAscii\n",
      "--------------------------------------------------------------------------------\n",
      "그들은\t\tPRON\tnpp+xsn+jxt\tdislocated\txxx\tTrue\tFalse\tFalse\tFalse\n",
      "이슬람교도들을\t\tNOUN\tncn+xsn+jco\tobj\txxxx\tTrue\tFalse\tFalse\tFalse\n",
      "동물에\t\tADV\tncn+jca\tobl\txxx\tTrue\tFalse\tFalse\tFalse\n",
      "빗대었습니다\t\tVERB\tpvg+ep+ef\tROOT\txxxx\tTrue\tFalse\tFalse\tFalse\n",
      ".\t\tPUNCT\tsf\tpunct\t.\tFalse\tFalse\tTrue\tTrue\n",
      "저는\t\tPRON\tnpp+jxt\tdislocated\txx\tTrue\tFalse\tFalse\tFalse\n",
      "이거슨\t\tPRON\tnpd+jxt\tdep\txxx\tTrue\tFalse\tFalse\tFalse\n",
      "좋아해요\t\tVERB\tpvg+ef\tROOT\txxxx\tTrue\tFalse\tFalse\tFalse\n",
      ".\t\tPUNCT\tsf\tpunct\t.\tFalse\tFalse\tTrue\tTrue\n"
     ]
    }
   ],
   "source": [
    "print_attributes(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그들은 이슬람교도들을 동물에 빗대었습니다.\n",
      "그들은 이슬람교도들을 동물에 빗대었습니다.\n",
      "그들은 이슬람교도들을 동물에 빗대었습니다.\n",
      "그들은 이슬람교도들을 동물에 빗대었습니다.\n",
      "그들은 이슬람교도들을 동물에 빗대었습니다.\n",
      "저는 이거슨 좋아해요.\n",
      "저는 이거슨 좋아해요.\n",
      "저는 이거슨 좋아해요.\n",
      "저는 이거슨 좋아해요.\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_id(example) -> list:\n",
    "    \"\"\"get the sentence index for each token\"\"\"\n",
    "    if example.has_annotation(\"SENT_START\"):\n",
    "        return [sent_id for sent_id, sent in enumerate(example.sents) for token in sent]\n",
    "    else:\n",
    "        return [0 for token in example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sent_id(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ea2b68fb3360e02997fb42a3e126fee4f6f46a4333cd08cb463da23fd84daeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
